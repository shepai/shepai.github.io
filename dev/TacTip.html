<!DOCTYPE html>
<html>
<head>
	<title>SHEP AI</title>
	<link rel="stylesheet" type="text/css" href="https://shepai.github.io/style.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="shortcut icon" href="https://shepai.github.io/assets/test1.ico" type="favicon/ico">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

</head>
<body class="backgroundC">
<div class="topnav">
<p class="title" align="center">Dexter Shepherd</p>
<a align="left" class="topnavlogo"><img src="https://shepai.github.io/assets/eyeT.png" width="50px" height="50px"></a>
<a class="topnavleft" href="https://shepai.github.io/index.html">Home</a>
<a class="topnavleft" href="https://shepai.github.io/index.html#about">About</a>
<a class="topnavleft" href="https://shepai.github.io/downloads.html">Downloads</a>
<a class="topnavleft" href="https://shepai.github.io/contact.html">Contact</a><br>
<!-- <a align="right" href="https://www.facebook.com/SHEP-AI-101118428133298/" class="fa fa-facebook"></a>
<a align="right" href="https://twitter.com/ai_shep" class="fa fa-twitter"></a>
<a align="right" href="https://www.instagram.com/shep.ai/" class="fa fa-instagram"></a>
<a align="right" href="https://www.youtube.com/channel/UCQr_MHaJ53feVK19lDKDxCQ?view_as=subscriber" class="fa fa-youtube"></a> -->
<a align="right" href="https://www.linkedin.com/in/dexter-shepherd-1a4a991b8/" class="fa fa-linkedin"></a>
<a align="right" href="https://scholar.google.com/citations?hl=en&user=hJSy6CYAAAAJ" class="ai ai-google-scholar ai-2x"></a>
<a align="right" href="https://github.com/shepai" class="fa fa-github"></a>
<a class="topnavright" class="search-container"><input id="searchbar" type="text" placeholder="Search.." name="search"></a>
<a href="#" class="topnavright"><i onclick="search()" class="icon fa fa-search"></i></a>
<script src="https://shepai.github.io/search.js">

</script>
<script src="https://shepai.github.io//tex-mml-chtml.js"></script>

</div>
	<!-- the main content -->
	<div class="main">
        <h1 class="headerText">TacTip construction</h1>
            Much of this work is published in <a href="https://doi.org/10.3390/s25164971">this paper</a> and my PhD thesis. 
        <br><br>
        The TacTip is a variant of optical tactile sensing. A camera faces a deformable body with markers. Based on the movement of these markers, texture, edges, or object properties can be classified. As a popular sensor in the field, we decided to construct one for further testing. 
        <br><br>

            The TacTip parts were 3D printed and consisted of a camera mount, main body, tip, and LED ring. 
            The camera mount was edited in CAD software to mount our webcam (Arducam USB wide-angle lens), 
            and the main body attached to the mount via a series of screws.

            
            <br><br>
            Different cameras were trialled, including autofocus, fisheye, wide-angle, and a standard webcam. 
            The standard webcam was a 720p Logitech webcam with a 55&deg; field of view.

            
            <br><br>
            The fisheye was an Arducam 2.4-megapixel camera with a fixed focus of 
            10&deg; (D) &times; 86&deg; (H) &times; 47&deg; (V). 
            Finally, we used an autofocus lens with the Arducam camera. We found that the autofocus was not reliable as it could change visuals while refocusing, leading to incoherent data. The Fisheye Arducam gave a high resolution image all the time. On the cheaper end we were unable to get images in focus. 
            <br><br>
            The files were printed using PLA filament on an industrial high-resolution printer (Stratasys Objet 30 Prime). Between the tip and the main body; there is a PCB of small LEDs (3.15V 1608 (0603) SMD) that provide light (45-180mcd each). The camera detects reflection of the LED light from the white painted tips on the surface of the skin. The PCB made use of 6 surface-mount LEDS and 6 120ohm surface-mount (0603) capacitors.
            The LED PCB was mounted in the rim of the main body, where the LEDs face the skin, to illuminate the pointers for the camera. The wires are pressed into a printed groove that leads to a hole allowing for power connection. The camera was mounted onto a custom 3D printed part that connected to a USB wide-angle lens webcam. The power of the LED ring was connected to the power of the USB camera for wiring efficiency.  
Soft-bodied skins are produced through silicone within a 3D-printed plastic mould. This mould was measured to fit the silicone inside a thin layer, with small holes in to create the tips that would later be painted white. The moulds were printed in high resolution to capture the 1mm diameter of the optical markers. The outcome would have a 42mm diameter exterior and 38mm diameter interior, where the skin has a 2mm thickness. 
Experimentation on thickness of the skins, and its effect on sensitivity was not the focus of this thesis, but remains fairly unexplored but has gone down to 1mm thickness. If the silicone is too thin there is an increased chance that it will break in the mould, therefore to ensure maximum durability we always used 2mm. 
Silicone was made and dyed black to prevent light interference or any more glare than the background levels. After being poured into the mould and left for twenty-four hours, a solid yet flexible TacTip was produced. The mould required a lubricant spray over the plastic layers to prevent the tips from getting pulled off while being removed from the mould. To paint the optical markers we used a thin layer of plastic sheet with acrylic paint. The sensor skin was turned inside out (so the marker positions were on the exterior) and gently dabbed over the paint.  
<br>
<div align="center">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/newDesign.png">

</div>
<br>
<div align="center">
    <img height="200px" src="https://github.com/shepai/RoboSkin/blob/main/Assets/images/regression.gif?raw=true">

</div>
<h2 class="headerText"> Alternative designs</h2>
<h3 class="headerText"> Alternative Bodies</h3>
We made some alternative designs of TacTip to expand upon the optical marker detection methods. The main principle of the TacTip sensor is the displacement of the optical markers. We devised a design that uses a flat silicone rectangle with a series of optical markers on the surface. With a camera facing this mesh, we could get tactile feedback over a surface area of 100x150mm. Each dot was 2mm in diameter, which is larger than the optical markers on the TacTip. 
Silicone was poured into the mould, then once set each dot was painted white (as we did with the original TacTip). Using a layer of clear silicone we made the flesh part of the skin, where a box of the same dimensions as the skin acts as the mould. Our skin is placed in the bottom of the box, with optical markers facing up. The clear silicone is poured onto the skin. We do this to protect the optical markers from having too much pressure making the paint wear down, and to provide a sponge-like texture for the skin to resist force. 

After making the silicone skin, a box was constructed to hold the camera, and above it a transparent plate between the skin, focal length and camera. Around the sides, it was taped with black to eliminate light interference through glare. The optical markers of the skin were painted white and the skin was placed in the box (without a gel layer).  
Combining the properties of the artificial skin and TacTip, we developed a full tactile foot that sensed both the bottom and side contact. Having a larger surface area of haptics on a leg would open up exploration into more complex styles of locomotion. 
<br>
<div align="center"align="center">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/thing.jpg">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/skin.png">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/tacLegVie.png">
</div>
<br>
We constructed the TacLeg from a 3D-printed mould. Initially, the skin would come out in reverse and need to be turned inside-out. We did this because when the mould is peeled off sometimes the marker points are destroyed. The inside-out method helps prevent this. 
These designs were limited by the cameras being able to focus at short ranges.   

<br><br>
<h3 class="headerText"> Alternative Tips</h3>
Using the standard TacTip body design, we can investigate a range of optical patterns. To what extent do optical markers influence the accuracy achievable by optical tactile sensors? In previous work, we used a standard array of 1mm optical markers, and more recently introduced large marker stripes in a new sensor morphology. Previous work has suggested that markers are not a requirement for optical tactile sensors. Despite this, the role of marker size, shape, and spatial distribution remains largely unexplored. This raises a fundamental question in tactile sensor design: to what extent does sensor morphology influence learning performance, and can alternative marker configurations, or even marker-free designs, achieve comparable accuracy while enabling lower-resolution sensing? We investigate the impact of marker size and frequency by experimenting with a range of optical marker designs. These include larger markers in various patterns-some composed of long lines, others of short segments, and some using non-linear or irregular shapes. While there was no strict design theme, the variety allowed us to qualitatively explore performance differences. In the future, algorithmically generated patterns could enable a more systematic and quantitative analysis. Our central question remains one of resolution: Can larger or differently shaped markers achieve similar accuracy to smaller, denser ones?
<br>
The new marker skins were attached to the rig setup and used to gather the same dataset as previous experiments. We apply Sobel filters and reduce the images by mean scaling. All the preprocessing steps and model parameters matched that of all previous experiments. We chose the CNN model due to its success in previous experiments.
<div align="center"align="center">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/0m.png">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/A.png">
</div>
<br>
<div align="center"align="center">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/C.png">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/D.png">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/newTip.png">
</div>
<br>
<div align="center"align="center">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/B.png">
    <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/morph/silicone.jpeg">
</div>
<h3 class="headerText"> Marker Prediction</h3>

We labelled and trained a regression model to predict the location of optical markers, in an effort to reduce the dimentionality of the data. 
<p>
    We employed the use of three regression models: Ridge Regression (with alpha = 1), Linear Regression, and Random Forest Regression (RFR) with depth 25. These models were trained 20 times with different random states, and the performance metric was determined by the average distance between labelled and predicted points divided by the diagonal pixel space \( \sqrt{h^2 + w^2} \) to normalise the error. We then subtracted this normalised error to obtain a percentage accuracy. The values were normalised to make them comparable across any image size <cite>florence2020dense, articlestats</cite>.
    </p>
    
    <p>
    Images were converted to greyscale as a preprocessing step, and a Sobel filter was applied to highlight the markers and reduce noise <cite>yergibay2024nusense</cite>. Results of this experiment are shown in Table 1.
    </p>
    <div align="center">
    <table border="1" style="border-collapse: collapse; text-align: center;">
      <caption><strong>Table 1:</strong> Comparison of regression models where test and training accuracy is averaged over 20 trials using the normalized accuracy metric and standard deviation.</caption>
      <thead>
        <tr>
          <th>Model</th>
          <th>Av Test Accuracy</th>
          <th>Av Train Accuracy</th>
          <th>Test Std</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Ridge</td>
          <td>87.902%</td>
          <td>99.99%</td>
          <td>0.72%</td>
        </tr>
        <tr>
          <td>Linear</td>
          <td>87.19%</td>
          <td>99.9%</td>
          <td>0.87%</td>
        </tr>
        <tr>
          <td>RFR</td>
          <td>76.79%</td>
          <td>82.41%</td>
          <td>1.55%</td>
        </tr>
      </tbody>
    </table>
</div>
    <p>
    The Ridge regression model performed the best out of the three models. We suspect some of the issues may stem from the large dimensionality of the input data, which also caused some of the models to take hours to train. We therefore employed Principal Component Analysis (PCA) to reduce the high-dimensional image data to 150 components. This smaller dataset was then passed into the models.
    </p>
    
    <p>
    Results of this experiment are shown in Table 2. This approach led to increased accuracy for the RFR model, but little difference in test accuracy for the other models.
    </p>
    <div align="center">
    <table border="1" style="border-collapse: collapse; text-align: center;">
      <caption><strong>Table 2:</strong> Comparison of regression models with PCA applied to the image data. Testing and training accuracy are averaged over 20 trials using the normalized accuracy metric and standard deviation.</caption>
      <thead>
        <tr>
          <th>Model</th>
          <th>Av Test Accuracy</th>
          <th>Av Train Accuracy</th>
          <th>Test Std</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Ridge</td>
          <td>87.95%</td>
          <td>80.64%</td>
          <td>0.48%</td>
        </tr>
        <tr>
          <td>Linear</td>
          <td>88.11%</td>
          <td>80.55%</td>
          <td>0.48%</td>
        </tr>
        <tr>
          <td>RFR</td>
          <td>85.83%</td>
          <td>88.92%</td>
          <td>1.13%</td>
        </tr>
      </tbody>
    </table>
</div>
    <p>
    Although the regression models show strong accuracy in point prediction, it raises questions as to whether this dimensionality reduction step provides benefits for classification tasks. There is an additional processing layer from image to point extraction. Furthermore, slight prediction errors may impact the accuracy of subsequent classification tasks.
    </p>
    
    <p>
    Another limitation of the point extraction method is that the labelling process was human-annotated. Errors may arise from fatigue or mistakes during point labelling. Despite these limitations, the models performed fairly well at predicting marker positions.
    </p>
    
<br>
<div align="center"><a href="https://www.kaggle.com/datasets/dextershepherd/tactip-optical-markers-labelled" target="_blank" style="text-decoration: none;">
    <button style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      background-color: #034485;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
    ">
      <!-- GitHub SVG Logo -->
      <svg height="20" width="20" viewBox="0 0 32 32" fill="white" xmlns="http://www.w3.org/2000/svg">
		<path d="M15.9 3.3h4.2v25.4h-4.2V17.9l-6.7 10.8H4.5l8.1-12.9L4.9 3.3h4.9l6.1 9.9V3.3z"/>
	  </svg>
  
      See Dataset
    </button>
  </a></div>
<br><br>
<div align="center"><a href="https://github.com/shepai/RoboSkin" target="_blank" style="text-decoration: none;">
    <button style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      background-color: #24292e;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
    ">
      <!-- GitHub SVG Logo -->
      <svg height="20" width="20" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
        0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13
        -.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87
        2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95
        0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21
        2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04
        2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82
        1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54
        1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013
        8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
      </svg>
  
      See Repo
    </button>
  </a></div>
      </div>
</div>
</body>
<script>


</script>

</html>
