<!DOCTYPE html>
<html>
<head>
	<title>SHEP AI</title>
	<link rel="stylesheet" type="text/css" href="https://shepai.github.io/style.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="shortcut icon" href="https://shepai.github.io/assets/test1.ico" type="favicon/ico">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>
<body class="backgroundC">
<div class="topnav">
<p class="title" align="center">Dexter Shepherd</p>
<a align="left" class="topnavlogo"><img src="https://shepai.github.io/assets/eyeT.png" width="50px" height="50px"></a>
<a class="topnavleft" href="https://shepai.github.io/index.html">Home</a>
<a class="topnavleft" href="https://shepai.github.io/index.html#about">About</a>
<a class="topnavleft" href="https://shepai.github.io/downloads.html">Downloads</a>
<a class="topnavleft" href="https://shepai.github.io/contact.html">Contact</a><br>
<!-- <a align="right" href="https://www.facebook.com/SHEP-AI-101118428133298/" class="fa fa-facebook"></a>
<a align="right" href="https://twitter.com/ai_shep" class="fa fa-twitter"></a>
<a align="right" href="https://www.instagram.com/shep.ai/" class="fa fa-instagram"></a>
<a align="right" href="https://www.youtube.com/channel/UCQr_MHaJ53feVK19lDKDxCQ?view_as=subscriber" class="fa fa-youtube"></a> -->
<a align="right" href="https://www.linkedin.com/in/dexter-shepherd-1a4a991b8/" class="fa fa-linkedin"></a>
<a align="right" href="https://scholar.google.com/citations?hl=en&user=hJSy6CYAAAAJ" class="ai ai-google-scholar ai-2x"></a>
<a align="right" href="https://github.com/shepai" class="fa fa-github"></a>
<a class="topnavright" class="search-container"><input id="searchbar" type="text" placeholder="Search.." name="search"></a>
<a href="#" class="topnavright"><i onclick="search()" class="icon fa fa-search"></i></a>
<script src="https://shepai.github.io/search.js">

</script>

</div>
	<!-- the main content -->
	<div class="main">
		<h1 class="headerText">Texture classification</h1>
		Much of this work is published in <a href="https://doi.org/10.3390/s25164971">this paper</a> and my PhD thesis.
		<h2 class="headerText">Rig Construction</h2>
		A 3 degrees of freedom rig (x-axis, y-axis, z-axis), of dimensions 300mm by 300mm by 150mm, was constructed to move a sensor around a 3D environment. This rig was used to gather texture data by stroking the sensor along 1.5mm straight lines in 100 different directions along evenly spaced radii of a semicircle centred on the original touching point. The sensor moved across fixed-down samples from the texture set. Recordings were gathered at various contact forces, using touching point forces of 0.0785 – 0.0824N, 1.9031 – 1.9228N, 3.0039 – 3.0235N and 4.336 – 4.367N. The dataset consists of 3000 items gathered over 15 textures. Each dataset item contains multiple sensor readings concatenated over the time that the sensor is in contact with the surface (approximately 10 seconds).
		<br>
        <div align="center"align="center">
            <img class="imageCircle2" width="100px" height="100px" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/rig.png">
        </div>
        <br>
		Moving the sensor across varying vectors shows the sensor's ability to classify the different textures with linear stroking movements, this is the most common way of gathering textural datasets. We also investigated how the sensor-classifier pairs performed on sensations they were not trained on. To do this, we collected a non-linear movement dataset by moving the sensor in circular movements of increasing radii (1cm, 1.75cm, 2.5cm) while increasing the contact force by approximately 17 grams on each iteration. We refer to this as the non-linear dataset. 

<br><br>
<div align="center"><a href="https://github.com/shepai/Rig-controller" target="_blank" style="text-decoration: none;">
    <button style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      background-color: #24292e;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
    ">
      <!-- GitHub SVG Logo -->
      <svg height="20" width="20" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
        0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13
        -.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87
        2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95
        0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21
        2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04
        2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82
        1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54
        1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013
        8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
      </svg>
  
      See Repo
    </button>
  </a></div>
		<h2 class="headerText">Dataset</h2>
Perhaps understandably, most papers on tactile sensing for texture classification have used textured materials that are easily accessible, and to date no standard texture set has been created. Past texture sets are often of different sizes which hampers comparability because, as shown in section, some texture classifiers perform well when distinguishing between small numbers of textures but degrade sharply as the number of classes increase. As established current datasets tend to use items that appear in the labs of the researchers. Often these include some form of carpet, hard materials and fabrics. Our texture dataset was designed to try and include commonalities between previous sets and to represent a range of material properties that a walking robot may come into contact with, such as indoors or flat outdoors surfaces. These included soft/hard bodies, coarse/smooth surfaces and surfaces with raised aspects. 
<br>
<div align="center"align="center">
	<img width="60%" src="https://raw.githubusercontent.com/shepai/shepai.github.io/refs/heads/master/assets/development/textures.png">
</div>
<br>
<div align="center"><a href="https://www.kaggle.com/datasets/dextershepherd/texture-tactip" target="_blank" style="text-decoration: none;">
    <button style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      background-color: #034485;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
    ">
      <!-- GitHub SVG Logo -->
      <svg height="20" width="20" viewBox="0 0 32 32" fill="white" xmlns="http://www.w3.org/2000/svg">
		<path d="M15.9 3.3h4.2v25.4h-4.2V17.9l-6.7 10.8H4.5l8.1-12.9L4.9 3.3h4.9l6.1 9.9V3.3z"/>
	  </svg>
  
      See Optical Dataset
    </button>
  </a></div>
  <br>
<div align="center"><a href="https://www.kaggle.com/datasets/dextershepherd/electrical-tactile-sensor" target="_blank" style="text-decoration: none;">
    <button style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 10px 16px;
      background-color: #034485;
      color: white;
      border: none;
      border-radius: 6px;
      font-size: 14px;
      font-weight: 500;
      cursor: pointer;
    ">
      <!-- GitHub SVG Logo -->
      <svg height="20" width="20" viewBox="0 0 32 32" fill="white" xmlns="http://www.w3.org/2000/svg">
		<path d="M15.9 3.3h4.2v25.4h-4.2V17.9l-6.7 10.8H4.5l8.1-12.9L4.9 3.3h4.9l6.1 9.9V3.3z"/>
	  </svg>
  
      See Electrical Dataset
    </button>
  </a></div>
<h2 class="headerText">Classification</h2>
<p>
	The results of the comparative texture classification experiments are shown in Table 1 for the various sensor–classifier combinations. It is clear from the table that the optical sensors have much higher accuracy across all classifier types compared to the electrical sensors. However, the electrical sensor combining the accelerometers and piezoelectric sensor offers good accuracy when using an LSTM classifier.
	</p>
	
	<p>
	The TacTip employing the new marker morphology performs slightly worse than with the original marker morphology, but is still very accurate. Overall, the best performing sensor is the silicone-filled TacTip, particularly in combination with CNN or LSTM classifiers. While SVM classifiers performed well, they took much longer to train than the other methods.
	</p>
	<div align="center">
	<table border="1" style="border-collapse: collapse; text-align: center;">
	  <caption>
		<strong>Table 1:</strong> Results of the comparative texture classification experiments using the TacTip (TT), with standard silicone tip (Sil), and the new morphology (NM), and the various PressTip (PT) configurations – piezoelectric denoted by P and accelerometer by A. The table shows the average accuracy for training and unseen test data across 20 trials, along with the maximum and standard deviation (std) of the results. Results for the optical sensors are for full image resolution. Additionally included are results for the TacTip using point prediction as a preprocessing step (PP), using the silicone tip with 133 markers.
	  </caption>
	  <thead>
		<tr>
		  <th>Sensor</th>
		  <th>Classifier</th>
		  <th>Average Test Accuracy</th>
		  <th>Average Train Accuracy</th>
		  <th>Std Test</th>
		  <th>Max Test</th>
		</tr>
	  </thead>
	  <tbody>
		<tr><td>TT Sil</td><td>SVM</td><td>99.96%</td><td>100%</td><td>0.0005</td><td>100%</td></tr>
		<tr><td>TT Sil</td><td>RFC</td><td>99.9%</td><td>99.98%</td><td>0.025</td><td>100%</td></tr>
		<tr><td>TT Sil</td><td>CNN</td><td>99.97%</td><td>99.99%</td><td>0</td><td>99.99%</td></tr>
		<tr><td>TT Sil</td><td>LSTM</td><td>98.1%</td><td>99.1%</td><td>0.018</td><td>99.9%</td></tr>
	
		<tr><td>TT NM</td><td>CNN</td><td>89.25%</td><td>90.31%</td><td>2.7</td><td>94.2%</td></tr>
		<tr><td>TT NM</td><td>LSTM</td><td>95.71%</td><td>96.28%</td><td>0.027</td><td>99.2%</td></tr>
	
		<tr><td>TT PP</td><td>RFC</td><td>98.44%</td><td>100%</td><td>0.59</td><td>99.33%</td></tr>
		<tr><td>TT PP</td><td>SVM</td><td>27.01%</td><td>30.22%</td><td>3.71</td><td>33.33%</td></tr>
		<tr><td>TT PP</td><td>LSTM</td><td>84.12%</td><td>84.5%</td><td>0.031</td><td>90.15%</td></tr>
		<tr><td>TT PP</td><td>ANN</td><td>86.16%</td><td>86.5%</td><td>0.026</td><td>90.48%</td></tr>
	
		<tr><td>PT P</td><td>SVM</td><td>70%</td><td>70.4%</td><td>0.01</td><td>71.6%</td></tr>
		<tr><td>PT A</td><td>SVM</td><td>44.75%</td><td>51.1%</td><td>0.025</td><td>49.1%</td></tr>
		<tr><td>PT A&amp;P</td><td>SVM</td><td>55.45%</td><td>57.8%</td><td>0.03</td><td>63.8%</td></tr>
	
		<tr><td>PT P</td><td>RFC</td><td>78.7%</td><td>99.96%</td><td>0.014</td><td>76.6%</td></tr>
		<tr><td>PT A</td><td>RFC</td><td>62.31%</td><td>100%</td><td>0.02</td><td>65.6%</td></tr>
		<tr><td>PT A&amp;P</td><td>RFC</td><td>89.6%</td><td>100%</td><td>0.01</td><td>99.2%</td></tr>
	
		<tr><td>PT P</td><td>ANN</td><td>66.5%</td><td>66.5%</td><td>0.768</td><td>67.5%</td></tr>
		<tr><td>PT A</td><td>ANN</td><td>39.57%</td><td>43.94%</td><td>0.759</td><td>41%</td></tr>
		<tr><td>PT A&amp;P</td><td>ANN</td><td>65.59%</td><td>64.84%</td><td>1.101</td><td>66.5%</td></tr>
	
		<tr><td>PT P</td><td>LSTM</td><td>77.37%</td><td>85.03%</td><td>0.67</td><td>78.6%</td></tr>
		<tr><td>PT A</td><td>LSTM</td><td>38.53%</td><td>42.1%</td><td>2.6</td><td>42%</td></tr>
		<tr><td>PT A&amp;P</td><td>LSTM</td><td>85.5%</td><td>90%</td><td>0.67</td><td>86.5%</td></tr>
	  </tbody>
	</table>
</div>

		<h2 class="headerText">Friction Prediction</h2>
		<p>
			Initially, we used a range of regression models (Linear, Ridge, Logistic) but found that the Random Forest classifier significantly outperformed the other regression models. Therefore, we only proceeded with the results of Ridge and Random Forest, which performed better than the Linear and Logistic models.
			</p>
			
			<p>
			Friction detection model performance was calculated using the mean squared error (MSE) between predicted and true values, with the mean calculated over all textures in the dataset. Although there is more noise in the voltage readings of the electrical sensors, the Random Forest regression model handled this well.
			</p>
			
			<p>
			Table 1 displays the results, showing a smaller error from the optical sensor. However, the relative performance of the electrical sensors is good and better than for the texture classification task. The TacTip values are more closely clustered around the line of best fit. Although the electrical sensor readings are noisier and more widely spread, their regression model produced a close match to the actual values. The results shown in Table 2 are for the clear-silicone TacTip and for various configurations of the electrical sensor (which also has a silicone tip).
			</p>
			
			<p>
			The Random Forest Regression classifier was found to be one of the quickest to train and highest performing for friction prediction across both sensors. We performed a comparison to evaluate how the different classifiers performed for the task of friction prediction using mean squared error as the metric. The same models were compared for both the electrical and optical sensors, where appropriate for the data type.
			</p>
			
			<p>
			The LSTM and CNN classifiers both used hidden layers of 350 nodes. The neural models had one output node and were trained with the Adam optimizer with a learning rate of 0.001. The results are shown in Table 1. The Random Forest was the highest performing model.
			</p>
			<div align="center">
			<table border="1" style="border-collapse: collapse; text-align: center;">
			  <caption>
				<strong>Table 1:</strong> Model Performance Across Different Sensors (averaged over 20 trials)
			  </caption>
			  <thead>
				<tr>
				  <th>Model</th>
				  <th>Sensor</th>
				  <th>Test MSE</th>
				  <th>Train MSE</th>
				</tr>
			  </thead>
			  <tbody>
				<tr><td>Ridge Regression</td><td>Optical</td><td>0.0002</td><td>0.0000</td></tr>
				<tr><td>Ridge Regression</td><td>Electrical</td><td>0.0238</td><td>0.0255</td></tr>
				<tr><td>CNN</td><td>Optical</td><td>0.0412</td><td>0.0366</td></tr>
				<tr><td>LSTM</td><td>Optical</td><td>0.0416</td><td>0.0368</td></tr>
				<tr><td>LSTM</td><td>Electrical</td><td>0.0368</td><td>0.0361</td></tr>
				<tr><td>Random Forest</td><td>Electrical</td><td>0.0030</td><td>0.0003</td></tr>
				<tr><td>Random Forest</td><td>Optical</td><td>0.0002</td><td>0.0000</td></tr>
			  </tbody>
			</table>
			</div>
			<br>
			<div align="center">
			<table border="1" style="border-collapse: collapse; text-align: center;">
			  <caption>
				<strong>Table 2:</strong> Friction prediction results showing mean squared error (MSE) between actual values and Random Forest regression model (RFR) predictions on the test data. Piezoelectric sensors are denoted as Piez and accelerometers as Acc. Results for the TacTip were gathered using the original image size. Each model was trialled 10 times.
			  </caption>
			  <thead>
				<tr>
				  <th>Sensor</th>
				  <th>Regression Model</th>
				  <th>Min MSE</th>
				  <th>Train MSE Average</th>
				</tr>
			  </thead>
			  <tbody>
				<tr><td>Piez</td><td>RFR</td><td>0.022</td><td>0.024</td></tr>
				<tr><td>Acc</td><td>RFR</td><td>0.019</td><td>0.021</td></tr>
				<tr><td>Acc &amp; Piez</td><td>RFR</td><td>0.018</td><td>0.021</td></tr>
				<tr><td>TacTip</td><td>RFR</td><td>0.0043</td><td>0.0049</td></tr>
			  </tbody>
			</table>
</div>
		<br><br>
		<div align="center"><a href="https://github.com/shepai/RoboSkin" target="_blank" style="text-decoration: none;">
			<button style="
			  display: inline-flex;
			  align-items: center;
			  gap: 8px;
			  padding: 10px 16px;
			  background-color: #24292e;
			  color: white;
			  border: none;
			  border-radius: 6px;
			  font-size: 14px;
			  font-weight: 500;
			  cursor: pointer;
			">
			  <!-- GitHub SVG Logo -->
			  <svg height="20" width="20" viewBox="0 0 16 16" fill="white" xmlns="http://www.w3.org/2000/svg">
				<path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
				0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13
				-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87
				2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95
				0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21
				2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04
				2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82
				1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54
				1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013
				8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
			  </svg>
		  
			  See Repo
			</button>
		  </a></div>
	</div>

	
</body>
<script>


</script>

</html>
